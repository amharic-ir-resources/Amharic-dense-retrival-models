{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49971 documents.\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "INPUT_FILE = \"./data/raw/amharic-news_dataset/amharic_news_classification_dataset.jsonl\"\n",
    "HOME_DIR = \"./dataset/processed/msmarco-amharic-news_dataset\"\n",
    "os.makedirs(HOME_DIR, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(input_file):\n",
    "    data = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Loaded {len(df)} documents.\")\n",
    "    return df\n",
    "\n",
    "df = load_dataset(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['headline', 'category', 'date', 'views', 'article', 'link', 'word_len',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ensure required columns exist\n",
    "assert \"article\" in df.columns, \"Missing column: 'article'\"\n",
    "assert \"headline\" in df.columns, \"Missing column: 'headline'\"\n",
    "assert \"label\" in df.columns, \"Missing column: 'label'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# print(df.isnull().sum())\n",
    "print(df[\"headline\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean articles\n",
    "df[\"article\"] = df[\"article\"].astype(str).str.strip().replace(r\"\\s+\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates. Remaining documents: 49839\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to generate MD5 hash\n",
    "def generate_md5(text):\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Apply MD5 hashing to remove duplicates\n",
    "df[\"hash\"] = df[\"article\"].apply(generate_md5)\n",
    "df = df.drop_duplicates(subset=\"hash\", keep=\"first\").drop(columns=[\"hash\"])  # Drop duplicate articles\n",
    "\n",
    "print(f\"Removed duplicates. Remaining documents: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Apply filtering conditions\n",
    "df = df[\n",
    "    (df[\"article\"].str.split().str.len() > 32) &  # Article must have more than 32 words\n",
    "    (df[\"article\"].str.split().str.len() < 256) &  # Article must have fewer than 256 words\n",
    "    (df[\"headline\"].str.split().str.len() < 64)  # Headline must have fewer than 64 words\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After filtering, 31148 documents remain.\n"
     ]
    }
   ],
   "source": [
    "print(f\" After filtering, {len(df)} documents remain.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31148\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure labels appear at least twice to avoid stratification issues\n",
    "df = df.groupby(\"label\").filter(lambda x: len(x) > 1)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique passages: 31148\n",
      "Index(['headline', 'category', 'date', 'views', 'article', 'link', 'word_len',\n",
      "       'label', 'passage_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Assign unique passage and query IDs\n",
    "df[\"passage_id\"] = range(len(df))\n",
    "passage_dict = dict(zip(df[\"article\"], df[\"passage_id\"]))\n",
    "print(f\"Number of unique passages: {df['passage_id'].nunique()}\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            headline category  \\\n",
      "6  በውድድር ወቅት በወረርሽኙ መከላከያ መመሪያዎች አተገባበር ላይ ስጋት እን...     ስፖርት   \n",
      "\n",
      "                date views                                            article  \\\n",
      "6  December 30, 2020     3  ብርሃን ፈይሳ አዲስ አበባ፡- ስፖርታዊ እንቅስቃሴና ውድድሮች በሚከናወኑበ...   \n",
      "\n",
      "                                link  word_len  label  passage_id  \n",
      "6  https://www.press.et/Ama/?p=38515       233      2           0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique passages:  31148\n",
      "Collection saved!,(31148 passages)\n"
     ]
    }
   ],
   "source": [
    "# Save Passage Collection\n",
    "collection_jsonl = os.path.join(HOME_DIR, \"collection.jsonl\")\n",
    "collection_tsv = os.path.join(HOME_DIR, \"collection.tsv\")\n",
    "i=0\n",
    "\n",
    "with open(collection_jsonl, \"w\", encoding=\"utf-8\") as f_jsonl, open(collection_tsv, \"w\", encoding=\"utf-8\") as f_tsv:\n",
    "    for _, row in df.iterrows():\n",
    "        json.dump({\"pid\": row[\"passage_id\"], \"text\": row[\"article\"]}, f_jsonl, ensure_ascii=False)\n",
    "        f_jsonl.write(\"\\n\")\n",
    "        f_tsv.write(f\"{row['passage_id']}\\t{row['article']}\\n\")\n",
    "        i+=1\n",
    "\n",
    "print(\"number of unique passages: \", i)\n",
    "print(f\"Collection saved!,({len(df)} passages)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    12845\n",
      "2     5584\n",
      "5     5431\n",
      "4     5264\n",
      "3     1949\n",
      "1       75\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 28033 | Dev: 3115\n"
     ]
    }
   ],
   "source": [
    "# Split into Train & Dev\n",
    "train_df, dev_df = train_test_split(df, test_size=0.1, stratify=df[\"label\"], random_state=42)\n",
    "print(f\"Train: {len(train_df)} | Dev: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['headline', 'category', 'date', 'views', 'article', 'link', 'word_len',\n",
      "       'label', 'passage_id', 'query_id'],\n",
      "      dtype='object')\n",
      "Index(['headline', 'category', 'date', 'views', 'article', 'link', 'word_len',\n",
      "       'label', 'passage_id', 'query_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.copy().reset_index(drop=True)\n",
    "dev_df = dev_df.copy().reset_index(drop=True)\n",
    "\n",
    "train_df[\"query_id\"] = range(len(train_df))  # Assign unique IDs for train\n",
    "dev_df[\"query_id\"] = range(len(train_df), len(train_df) + len(dev_df))  # Continue IDs for dev\n",
    "print(train_df.columns)\n",
    "print(dev_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    11560\n",
      "2     5026\n",
      "5     4888\n",
      "4     4738\n",
      "3     1754\n",
      "1       67\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    1285\n",
      "2     558\n",
      "5     543\n",
      "4     526\n",
      "3     195\n",
      "1       8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"label\"].value_counts())\n",
    "print(dev_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label Distribution:\n",
      " label\n",
      "0    0.412371\n",
      "2    0.179289\n",
      "5    0.174366\n",
      "4    0.169015\n",
      "3    0.062569\n",
      "1    0.002390\n",
      "Name: proportion, dtype: float64\n",
      "Dev Label Distribution:\n",
      " label\n",
      "0    0.412520\n",
      "2    0.179133\n",
      "5    0.174318\n",
      "4    0.168860\n",
      "3    0.062600\n",
      "1    0.002568\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Label Distribution:\\n\", train_df[\"label\"].value_counts(normalize=True))\n",
    "print(\"Dev Label Distribution:\\n\", dev_df[\"label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"UNKNOWN_QUERY\"\n",
    "    return text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_queries_and_qrels(dataframe, split_name):\n",
    "    queries_jsonl = os.path.join(HOME_DIR, f\"queries_{split_name}.jsonl\")\n",
    "    queries_tsv = os.path.join(HOME_DIR, f\"queries_{split_name}.tsv\")\n",
    "    qrels_tsv = os.path.join(HOME_DIR, f\"qrels_{split_name}.tsv\")\n",
    "    \n",
    "    with open(queries_jsonl, \"w\", encoding=\"utf-8\") as f_jsonl, open(queries_tsv, \"w\", encoding=\"utf-8\") as f_tsv, open(qrels_tsv, \"w\", encoding=\"utf-8\") as f_qrels:\n",
    "        for _, row in dataframe.iterrows():\n",
    "            cleaned_query = clean_query(row[\"headline\"])\n",
    "            json.dump({\"query_id\": row[\"query_id\"], \"headline\": cleaned_query}, f_jsonl, ensure_ascii=False)\n",
    "            f_jsonl.write(\"\\n\")\n",
    "            f_tsv.write(f\"{row['query_id']}\\t{cleaned_query}\\n\")\n",
    "            f_qrels.write(f\"{row['query_id']}\\t0\\t{row['passage_id']}\\t1\\n\")\n",
    "\n",
    "    print(f\"Saved {split_name} queries and qrels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate queries or passages found!\n"
     ]
    }
   ],
   "source": [
    "assert train_df[\"query_id\"].nunique() == len(train_df), \" Duplicate queries in train set!\"\n",
    "assert dev_df[\"query_id\"].nunique() == len(dev_df), \" Duplicate queries in dev set!\"\n",
    "assert df[\"passage_id\"].nunique() == len(df), \" Duplicate passages in dataset!\"\n",
    "\n",
    "print(\"No duplicate queries or passages found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train queries and qrels.\n",
      "Saved dev queries and qrels.\n"
     ]
    }
   ],
   "source": [
    "#Save train and dev datasets\n",
    "save_queries_and_qrels(train_df, \"train\")\n",
    "save_queries_and_qrels(dev_df, \"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amharic_colbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
